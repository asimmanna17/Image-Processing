{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd38df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from Joint_HDRDN import Joint_HDRDN\n",
    "from alignment_network import Alignment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2cd2e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asim/.conda/envs/asimenv/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model = Joint_HDRDN(n_channel =8, out_channel = 4, embed_dim = 60, depths=[4, 4, 4])\n",
    "full_model = nn.DataParallel(full_model)\n",
    "# 2. Load state_dict from file\n",
    "state_dict = torch.load('Joint_HDRDN.pth', map_location='cuda')  # load directly to GPU\n",
    "\n",
    "full_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01549ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_state_dict = full_model.state_dict()\n",
    "\n",
    "# Create a new dictionary with only align_head, att1, att2, conv_first\n",
    "new_state_dict = {k: v for k, v in full_state_dict.items() if any(part in k for part in ['align_head', 'att1', 'att2', 'conv_first'])}\n",
    "torch.save(new_state_dict, 'alignment_only_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b1d4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentOnlyModel(nn.Module):\n",
    "    def __init__(self, pretrained_alignment, embed_dim= 60):\n",
    "        super(AlignmentOnlyModel, self).__init__()\n",
    "\n",
    "        # Load the pre-trained alignment parts\n",
    "        self.align_head = pretrained_alignment.align_head\n",
    "        self.att1 = pretrained_alignment.att1\n",
    "        self.att2 = pretrained_alignment.att2\n",
    "        self.conv_first = pretrained_alignment.conv_first\n",
    "        self.embed_dim = embed_dim\n",
    "        # New layers on top\n",
    "        self.extra_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, 64, kernel_size=3, padding=1),   # 96 -> 64 channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),    # 64 -> 32 channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 3, kernel_size=3, padding=1)      # 32 -> 3 channels (final output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        # --- Alignment Part ---\n",
    "        f1_att, f2, f3_att, f1, f3 = self.align_head(x1, x2, x3)\n",
    "        f1_att = f2 + f1_att\n",
    "        f3_att = f2 + f3_att\n",
    "\n",
    "        f1_att = self.att1(f1_att, f2) * f1_att\n",
    "        f3_att = self.att2(f3_att, f2) * f3_att\n",
    "\n",
    "        x = self.conv_first(torch.cat((f1_att, f2, f3_att), axis=1))  # shape (batch, 96, H, W)\n",
    "\n",
    "        # --- New Layers ---\n",
    "        out = self.extra_layers(x)  # Final output with 3 channels\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a107ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the saved small .pth\n",
    "pretrained_alignment = Alignment(n_channel =8, out_channel = 4, embed_dim = 60, depths=[4, 4, 4])  # same structure\n",
    "pretrained_alignment = nn.DataParallel(pretrained_alignment).cuda()\n",
    "pretrained_alignment.load_state_dict(torch.load('alignment_only_weights.pth'))\n",
    "\n",
    "# 2. Create new network\n",
    "model = AlignmentOnlyModel(pretrained_alignment.module, embed_dim = 60)  # .module because of DataParallel\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f7cf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 992, 992) (4, 992, 992)\n"
     ]
    }
   ],
   "source": [
    "npypath = '/data/asim/ISP/HDR_transformer/data/RAW/raw-2022-0606-2151-4147.npz'\n",
    "imdata = np.load(npypath)\n",
    "\n",
    "sht = imdata['sht']\n",
    "mid = imdata['mid']\n",
    "lng = imdata['lng']\n",
    "hdr = imdata['hdr']\n",
    "\n",
    "crop_size = 992\n",
    "H, W = hdr.shape[1], hdr.shape[2]\n",
    "start_h = (H - crop_size) // 2\n",
    "start_w = (W - crop_size) // 2\n",
    "\n",
    "sht_crop = sht[:, start_h:start_h+crop_size, start_w:start_w+crop_size]\n",
    "mid_crop = mid[:, start_h:start_h+crop_size, start_w:start_w+crop_size]\n",
    "lng_crop = lng[:, start_h:start_h+crop_size, start_w:start_w+crop_size]\n",
    "hdr_crop = hdr[:, start_h:start_h+crop_size, start_w:start_w+crop_size]\n",
    "\n",
    "print(sht_crop.shape, hdr_crop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9729903c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 992, 992]), torch.Size([1, 4, 992, 992]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_tensor(np_array):\n",
    "    t = torch.from_numpy(np_array).float()\n",
    "    return t\n",
    "\n",
    "im1 = to_tensor(sht_crop).to(device).unsqueeze(0)\n",
    "im2 = to_tensor(mid_crop).to(device).unsqueeze(0)\n",
    "im3 = to_tensor(lng_crop).to(device).unsqueeze(0)\n",
    "ref_hdr = to_tensor(hdr_crop).to(device).unsqueeze(0)\n",
    "im1[:, :4, :, :].shape, ref_hdr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d3fec1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 992, 992])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generate_hdr = model(im1, im2, im3)\n",
    "generate_hdr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162440ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
